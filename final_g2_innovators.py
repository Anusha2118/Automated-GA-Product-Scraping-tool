# -*- coding: utf-8 -*-
"""Final_g2_innovators.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1JrnpoUhmnImXW43B1WQfKj8pMMOMWgph
"""

#team: g2 innovators
import sched
import time
import pandas as pd
import aiohttp
import asyncio
from bs4 import BeautifulSoup
import nest_asyncio
import requests
import datetime

# Allow running asyncio code in environments with an active event loop
nest_asyncio.apply()
duration=0

# Asynchronously fetches the HTML content of a given URL using an aiohttp session.
async def fetch(session, url):
    async with session.get(url) as response:
        return await response.text()

# Gets the catgeories from betalist and stores it as a list
async def get_categories():
    url = "https://betalist.com/topics"
    async with aiohttp.ClientSession() as session:
        html = await fetch(session, url)
        soup = BeautifulSoup(html, 'html.parser')
        categories = []
        for category in soup.find_all('a', class_='flex items-center gap-1 px-2 hover:bg-gray-100 group gap-4 hover:-my-[1px]'):
            category_href = category['href']
            category_name = category_href.split('/')[-1]
            categories.append(category_name)
        return categories

# Asynchronously scrapes startup data from the Betalist website for a given category.
# Returns a list of dictionaries containing startup information.
async def scrape_category(session, category):
    page_num = 1
    startup_data = []

    while True:
        url = f"https://betalist.com/topics/{category}?page={page_num}"
        html = await fetch(session, url)
        soup = BeautifulSoup(html, 'html.parser')
        articles = soup.find_all('div', class_='startupCard__details__text mt-3 text-base')

        if not articles:
            break

        for article in articles:
            startup_link = article.find('a')
            if startup_link:
                startup_name = startup_link.text.strip()
                block_text = article.text.strip().replace(startup_name, '').strip()
                startup_data.append({'Category': category, 'Name': startup_name, 'Block Text': block_text})

        page_num += 1

    return startup_data

async def scrape_categories(categories):
    async with aiohttp.ClientSession() as session:
        tasks = [scrape_category(session, category) for category in categories]
        results = await asyncio.gather(*tasks)
        return results

# Searches for products in batches using the G2 API.
# Returns lists of found and not found products.
def search_products_in_batches(product_names, batch_size=10):
    url = 'https://data.g2.com/api/v1/products'
    headers = {
        'Authorization': 'Token token=c9bb10f81b8415efeea89328f4f158f77f835308a2a955a2961f5474197e7801',
        'Content-Type': 'application/vnd.api+json'
    }

    found_products = []
    not_found_products = []
    for i in range(0, len(product_names), batch_size):
        batch = product_names[i:i + batch_size]
        batch_request_data = {'filter[name]': ','.join(batch)}
        response = requests.get(url, headers=headers, params=batch_request_data)
        if response.status_code == 200:
            data = response.json()
            products = data.get('data', [])
            for product_name in batch:
                matched_products = [product['attributes']['name'] for product in products if
                                    product_name.lower() == product['attributes']['name'].lower()]
                if matched_products:
                    found_products.extend(matched_products)
                else:
                    not_found_products.append(product_name)
        else:
            print(f"Failed to retrieve products. Status code: {response.status_code}")
    return found_products, not_found_products

# Reads product names from a CSV file.
# Returns a list of product names.
def read_product_names_from_csv(csv_file):
    df = pd.read_csv(csv_file)
    return df['Name'].tolist()

# Searches for products from the csv file created by calling the function, saves found and not found products in csv files
def search_products_in_csv(csv_file, batch_size):
    found_products_all_files = []
    not_found_products_all_files = []
    product_names = read_product_names_from_csv(csv_file)
    found_products, not_found_products = search_products_in_batches(product_names, batch_size)
    found_products_all_files.extend(found_products)
    not_found_products_all_files.extend(not_found_products)
    df_found = pd.DataFrame({'Found Product Name': found_products})
    df_found.to_csv(f'found_{csv_file}', mode='a', index=False, header=False)
    print(f"CSV file created: found_{csv_file}")
    df_not_found = pd.DataFrame({'Not Found Product Name': not_found_products})
    df_not_found.to_csv(f'not_found_{csv_file}', mode='a', index=False, header=False)
    print(f"CSV file created: not_found_{csv_file}")
# Scrapes startup data from Betalist for all categories, saves it to a CSV file, and searches for products.
async def main():
    def calculate_seconds(period):
      if period == 'daily':
          return 24 * 60 * 60
      elif period == 'weekly':
          return 7 * 24 * 60 * 60
      elif period == 'monthly':
          return 30 * 24 * 60 * 60
      elif period == '10minutes':
          return 10*60
      else:
          return "Invalid input. Please choose '10minutes','daily', 'weekly', or 'monthly'."
    period = input("Choose between 10minutes, daily, weekly, or monthly: ")
    duration=calculate_seconds(period)
    items = await get_categories()
    startup_data = await scrape_categories(items)
    flat_list = [item for sublist in startup_data for item in sublist]
    timestamp = datetime.datetime.now().strftime('%Y-%m-%d')
    csv_filename = f'betalist_data_{timestamp}.csv'
    df = pd.DataFrame(flat_list)
    df.to_csv(csv_filename, index=False)
    print(f"Scraping complete. Data saved to '{csv_filename}'.")
    return duration

# Runs a scheduler that periodically executes the main scraping and product search tasks.
def run_scheduler():
    s = sched.scheduler(time.time, time.sleep)
    def scheduled_task():
        duration=asyncio.run(main())
        timestamp = datetime.datetime.now().strftime('%Y-%m-%d')
        csv_filename = f'betalist_data_{timestamp}.csv'
        search_products_in_csv(csv_filename, batch_size=60)
        print("Next Cycle starts after 10 minutes:")
        s.enter(duration, 1, scheduled_task)

    s.enter(0, 1, scheduled_task)
    s.run()

if __name__ == "__main__":
    run_scheduler()

